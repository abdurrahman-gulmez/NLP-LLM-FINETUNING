{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iFmFs713QVW4",
        "outputId": "bf4b1e0b-e04e-4a92-9902-890f4a9ccb28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow\n",
            "  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 22.0.0\n",
            "    Uninstalling pyarrow-22.0.0:\n",
            "      Successfully uninstalled pyarrow-22.0.0\n",
            "Successfully installed pyarrow-22.0.0\n",
            "Requirement already satisfied: datasets==4.3.0 in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.3.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets==4.3.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets==4.3.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.3.0) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets==4.3.0) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes -q\n",
        "!pip install --upgrade --force-reinstall pyarrow\n",
        "!pip install datasets==4.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krTlGakcM2ei",
        "outputId": "1b22ff9c-7d72-4fe5-a7c4-63c0939b1999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/datasets\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD5_stV3M69w",
        "outputId": "8ba67744-d94d-4324-b525-0799980e0691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CodeGen-Deep-5K' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/Naholav/CodeGen-Deep-5K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWyr2NJLOD4Z",
        "outputId": "460c32c0-0cd2-40dd-a0af-504bae9c1a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CodeGen-Diverse-5K' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/Naholav/CodeGen-Diverse-5K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJeGXV9-OP-4",
        "outputId": "cda5e3c1-e618-4c99-e401-3554524cbde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/model\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmYsPUf6ORmF",
        "outputId": "bd044ced-63a7-4620-a198-ee0affad83ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Qwen2.5-Coder-1.5B-Instruct' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tYBbIbudEyU",
        "outputId": "eb7e4972-a610-4dba-b73c-5f4969027811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "petq00JAOUD_",
        "outputId": "f92f5d91-daab-4ea5-bda2-843182904292",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from transformers.trainer_callback import TrainerCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5IHF9zJIo0V"
      },
      "outputs": [],
      "source": [
        "TRAINING_MODE = \"diverse\"\n",
        "\n",
        "USE_REASONING = True\n",
        "\n",
        "DEEP_DATASET = \"Naholav/CodeGen-Deep-5K\"\n",
        "DIVERSE_DATASET = \"Naholav/CodeGen-Diverse-5K\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK_d2Ne5Lcbm"
      },
      "outputs": [],
      "source": [
        "if USE_REASONING:\n",
        "  SYSTEM_PROMPT = \"You are an expert Python programmer. Please read the problem carefully before writing any Python code.\"\n",
        "else:\n",
        "  SYSTEM_PROMPT = \"You are an expert programmer. Use <think> tags for reasoning before writing code.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ZgrZsiLe8v"
      },
      "outputs": [],
      "source": [
        "LORA_R = 32\n",
        "LORA_ALPHA = 64  # r * 2\n",
        "LORA_DROPOUT = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MhdePrmLgtt"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 2e-4\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "GRADIENT_ACCUMULATION = 1\n",
        "MAX_LENGTH = 1024 if not USE_REASONING else 8192\n",
        "\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.04\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qBZ0Jw6LfXm"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "DIRECTORY_PATH = \"/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA\"\n",
        "OUTPUT_DIR = f\"{DIRECTORY_PATH}/training_models/{TRAINING_MODE}/{TRAINING_MODE}_{timestamp}\"\n",
        "LOG_DIR = f\"{DIRECTORY_PATH}/logs/{TRAINING_MODE}/{TRAINING_MODE}_{timestamp}\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxRczUCPDc5"
      },
      "outputs": [],
      "source": [
        "class LogCallback(TrainerCallback):\n",
        "    \"\"\"Loss'larÄ± hem ekrana hem dosyaya yaz\"\"\"\n",
        "    def __init__(self, log_file):\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                log_entry = {\n",
        "                    \"step\": state.global_step,\n",
        "                    \"epoch\": round(state.epoch, 2) if state.epoch else 0,\n",
        "                    **logs\n",
        "                }\n",
        "                f.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "            if \"loss\" in logs:\n",
        "                print(f\"ðŸ“Š [Step {state.global_step}] Train Loss: {logs['loss']:.4f}\")\n",
        "            if \"eval_loss\" in logs:\n",
        "                print(f\"ðŸ“ˆ [Step {state.global_step}] Eval Loss: {logs['eval_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVuWgF-5PkHQ",
        "outputId": "bd03e49b-329a-4273-a0cb-70ce96541aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model yÃ¼kleniyor...\n",
            "==((====))==  Unsloth 2025.12.1: Fast Qwen2 patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model yÃ¼klendi!\n",
            "Max sequence length: 8192\n",
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Model yÃ¼kleniyor...\")\n",
        "\n",
        "max_seq_length = MAX_LENGTH\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-Coder-1.5B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=False,\n",
        ")\n",
        "\n",
        "print(\"Model yÃ¼klendi!\")\n",
        "print(f\"Max sequence length: {max_seq_length}\")\n",
        "print(f\"Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA8LQCh_lkku",
        "outputId": "701bbfc4-dae6-4a2d-f130-161944b06cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAÅžLANGIÃ‡ MODELÄ° CEVABI:\n",
            "system\n",
            "You are an expert programmer. Use <think> tags for reasoning before writing code.\n",
            "user\n",
            "Write a Python function to check if a number is prime.\n",
            "assistant\n",
            "To check if a number is prime, we need to determine if it has any divisors other than 1 and itself. Here's a step-by-step approach to implement this in Python:\n",
            "\n",
            "1. Define a function `is_prime` that takes an integer `num` as input.\n",
            "2. Check if the number is less than 2. If so, return `False` because numbers less than 2 are not prime.\n",
            "3. Iterate from 2 up to the square root of `num`. For each number `i`, check if `num` is divisible by `i`.\n",
            "4. If `num` is divisible by any number `i` (other than 1 and itself), return `False`.\n",
            "5. If no divisors are found, return `True`.\n",
            "\n",
            "Here's the implementation of the `is_prime` function:\n",
            "\n",
            "```python\n",
            "import math\n",
            "\n",
            "def is_prime(num):\n",
            "    # Step 1: Check if num is less than 2\n",
            "    if num < 2:\n",
            "        return False\n",
            "    \n",
            "    # Step 2: Check for factors from 2 to the square root of num\n",
            "    for i in range(2, int(math.sqrt(num)) + 1):\n",
            "        if num % i == 0:\n",
            "            return False\n",
            "    \n",
            "    # Step 3: If no factors were found, return True\n",
            "    return True\n",
            "\n",
            "# Example usage:\n",
            "print(is_prime(29))  # Output: True\n",
            "print(is_prime(10))  # Output: False\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "- **Step 1**: The function first checks if `num` is less than 2. If it is, the function immediately returns `False` because numbers less than 2 are not prime.\n",
            "- **Step 2**: The function then iterates through all integers from 2 up to the square root of `num`. This is efficient because a larger factor of `num` must be a multiple of a smaller factor that has already been checked.\n",
            "- **Step 3**: For each integer `i`, the function checks if `num` is divisible by `i` using the modulus operator (`%`). If `num` is divisible by `i`, it means `i` is a divisor of `num`, and the function returns `False`.\n",
            "- **Step 4**: If the loop completes without finding any divisors, the function returns `True`, indicating that `num` is prime.\n",
            "\n",
            "This method ensures that we efficiently check for primality by reducing the number of iterations needed compared to checking every number up to `num`.\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "if not USE_REASONING:\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Please read the problem carefully before writing any Python code.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Write a Python function to check if a number is prime.\"}\n",
        "  ]\n",
        "else:\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are an expert programmer. Use <think> tags for reasoning before writing code.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Write a Python function to check if a number is prime.\"}\n",
        "  ]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=1024)\n",
        "print(\"BAÅžLANGIÃ‡ MODELÄ° CEVABI:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q17ADAfSYczN",
        "outputId": "1d90fa87-d7ca-4cba-b1f1-c251f9f82a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LoRA uygulanÄ±yor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.12.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA uygulandÄ±!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nLoRA uygulanÄ±yor...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "print(\"LoRA uygulandÄ±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNelwztxYiYt",
        "outputId": "a7636eb2-0d3c-4e37-bc0d-2e925d4c40ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset yÃ¼kleniyor...\n",
            "Dataset: Naholav/CodeGen-Diverse-5K\n",
            "Train: 4500 Ã¶rnekler\n",
            "Test: 500 Ã¶rnekler\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset yÃ¼kleniyor...\")\n",
        "dataset_name = DEEP_DATASET if TRAINING_MODE == \"deep\" else DIVERSE_DATASET\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"Train: {len(dataset['train'])} Ã¶rnekler\")\n",
        "print(f\"Test: {len(dataset['test'])} Ã¶rnekler\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3CTH1WHPt6W"
      },
      "outputs": [],
      "source": [
        "def formatting_func(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"] if USE_REASONING else examples[\"solution\"]\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for input_text, output_text in zip(inputs, outputs):\n",
        "        text = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
        "        text += f\"<|im_start|>user\\n{input_text}<|im_end|>\\n\"\n",
        "        text += f\"<|im_start|>assistant\\n{output_text}<|im_end|>\"\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8yKn8vrY_AW",
        "outputId": "b4914d8e-e0fa-45dd-e410-cdfb4ce2b0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ayarlarÄ± hazÄ±rlanÄ±yor...\n",
            "Effective batch size: 16\n",
            "Precision: BF16\n"
          ]
        }
      ],
      "source": [
        "print(\"Training ayarlarÄ± hazÄ±rlanÄ±yor...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "\n",
        "    # Optimizer & Scheduler\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    optim=\"adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # Logging & Saving\n",
        "    logging_steps=20,\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Evaluation & Model Selection\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    # Optimization\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # Misc\n",
        "    report_to=\"tensorboard\",\n",
        "    seed=SEED,\n",
        "    dataloader_num_workers=2,\n",
        ")\n",
        "\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"Precision: {'BF16' if is_bfloat16_supported() else 'FP16'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOxxUitWP0f1",
        "outputId": "14b9464a-3f71-4345-a0c8-edd14002ba56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainer hazÄ±rlanÄ±yor...\n",
            "Trainer hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTrainer hazÄ±rlanÄ±yor...\")\n",
        "log_file = os.path.join(LOG_DIR, \"training_log.jsonl\")\n",
        "log_callback = LogCallback(log_file)\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args,\n",
        "    callbacks=[log_callback, early_stopping],\n",
        "    packing=False,\n",
        "    formatting_func=formatting_func\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"training_mode\": TRAINING_MODE,\n",
        "    \"dataset\": dataset_name,\n",
        "    \"use_reasoning\": USE_REASONING,\n",
        "    \"base_model\": \"Qwen2.5-Coder-1.5B-Instruct\",\n",
        "    \"framework\": \"unsloth\",\n",
        "\n",
        "    \"lora_config\": {\n",
        "        \"r\": LORA_R,\n",
        "        \"alpha\": LORA_ALPHA,\n",
        "        \"dropout\": LORA_DROPOUT,\n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    },\n",
        "\n",
        "    \"training_config\": {\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"gradient_accumulation\": GRADIENT_ACCUMULATION,\n",
        "        \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
        "        \"max_length\": MAX_LENGTH,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"warmup_ratio\": WARMUP_RATIO,\n",
        "        \"optimizer\": \"adamw_8bit\",\n",
        "        \"scheduler\": \"cosine\",\n",
        "        \"early_stopping_patience\": 2,\n",
        "        \"seed\": SEED\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"config.json\"), 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"Trainer hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xkoiChLlidgT",
        "outputId": "cae8a04f-a857-4375-e6fd-121648c49db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EÄžÄ°TÄ°M BAÅžLIYOR!\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Total Memory: 79.32 GB\n",
            "Compute Capability: 8.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 4,500 | Num Epochs = 3 | Total steps = 846\n",
            "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 36,929,536 of 1,580,643,840 (2.34% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='700' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [700/846 21:47 < 04:33, 0.53 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.988000</td>\n",
              "      <td>0.978054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.977300</td>\n",
              "      <td>0.953316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.900300</td>\n",
              "      <td>0.947177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.880800</td>\n",
              "      <td>0.939701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.863700</td>\n",
              "      <td>0.930930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.790600</td>\n",
              "      <td>0.945893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.770900</td>\n",
              "      <td>0.945679</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š [Step 20] Train Loss: 1.3358\n",
            "ðŸ“Š [Step 40] Train Loss: 1.0978\n",
            "ðŸ“Š [Step 60] Train Loss: 1.0624\n",
            "ðŸ“Š [Step 80] Train Loss: 0.9913\n",
            "ðŸ“Š [Step 100] Train Loss: 0.9880\n",
            "ðŸ“ˆ [Step 100] Eval Loss: 0.9781\n",
            "ðŸ“Š [Step 120] Train Loss: 1.0145\n",
            "ðŸ“Š [Step 140] Train Loss: 0.9996\n",
            "ðŸ“Š [Step 160] Train Loss: 0.9731\n",
            "ðŸ“Š [Step 180] Train Loss: 0.9698\n",
            "ðŸ“Š [Step 200] Train Loss: 0.9773\n",
            "ðŸ“ˆ [Step 200] Eval Loss: 0.9533\n",
            "ðŸ“Š [Step 220] Train Loss: 0.9869\n",
            "ðŸ“Š [Step 240] Train Loss: 0.9675\n",
            "ðŸ“Š [Step 260] Train Loss: 0.9610\n",
            "ðŸ“Š [Step 280] Train Loss: 0.9646\n",
            "ðŸ“Š [Step 300] Train Loss: 0.9003\n",
            "ðŸ“ˆ [Step 300] Eval Loss: 0.9472\n",
            "ðŸ“Š [Step 320] Train Loss: 0.8559\n",
            "ðŸ“Š [Step 340] Train Loss: 0.8600\n",
            "ðŸ“Š [Step 360] Train Loss: 0.8487\n",
            "ðŸ“Š [Step 380] Train Loss: 0.8802\n",
            "ðŸ“Š [Step 400] Train Loss: 0.8808\n",
            "ðŸ“ˆ [Step 400] Eval Loss: 0.9397\n",
            "ðŸ“Š [Step 420] Train Loss: 0.8976\n",
            "ðŸ“Š [Step 440] Train Loss: 0.8969\n",
            "ðŸ“Š [Step 460] Train Loss: 0.8426\n",
            "ðŸ“Š [Step 480] Train Loss: 0.8783\n",
            "ðŸ“Š [Step 500] Train Loss: 0.8637\n",
            "ðŸ“ˆ [Step 500] Eval Loss: 0.9309\n",
            "ðŸ“Š [Step 520] Train Loss: 0.8449\n",
            "ðŸ“Š [Step 540] Train Loss: 0.8722\n",
            "ðŸ“Š [Step 560] Train Loss: 0.8878\n",
            "ðŸ“Š [Step 580] Train Loss: 0.8116\n",
            "ðŸ“Š [Step 600] Train Loss: 0.7906\n",
            "ðŸ“ˆ [Step 600] Eval Loss: 0.9459\n",
            "ðŸ“Š [Step 620] Train Loss: 0.7687\n",
            "ðŸ“Š [Step 640] Train Loss: 0.8008\n",
            "ðŸ“Š [Step 660] Train Loss: 0.7708\n",
            "ðŸ“Š [Step 680] Train Loss: 0.7721\n",
            "ðŸ“Š [Step 700] Train Loss: 0.7709\n",
            "ðŸ“ˆ [Step 700] Eval Loss: 0.9457\n",
            "EÄžÄ°TÄ°M TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "print(\"EÄžÄ°TÄ°M BAÅžLIYOR!\")\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "print(f\"GPU: {gpu_stats.name}\")\n",
        "print(f\"Total Memory: {round(gpu_stats.total_memory / 1024 / 1024 / 1024, 2)} GB\")\n",
        "print(f\"Compute Capability: {gpu_stats.major}.{gpu_stats.minor}\\n\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"EÄžÄ°TÄ°M TAMAMLANDI!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlwxrmSgMlUj",
        "outputId": "df109473-c31e-4747-fc85-1ba212439dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model kaydediliyor...\n",
            "LoRA adapter: /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/lora_adapter\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nModel kaydediliyor...\")\n",
        "lora_dir = os.path.join(DIRECTORY_PATH, \"lora_adapter\")\n",
        "model.save_pretrained(lora_dir)\n",
        "tokenizer.save_pretrained(lora_dir)\n",
        "print(f\"LoRA adapter: {lora_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmzWfMmdMzCh",
        "outputId": "1e2054ff-7ae7-4e4d-9cba-196983d323ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Merged model oluÅŸturuluyor...\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Copying 1 files from cache to `/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/training_models/diverse/diverse_20251208_171913/merged_model`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied all 1 files from cache to `/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/training_models/diverse/diverse_20251208_171913/merged_model`\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 8473.34it/s]\n",
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/training_models/diverse/diverse_20251208_171913/merged_model`\n",
            "Merged model (16bit): /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/training_models/diverse/diverse_20251208_171913/merged_model\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMerged model oluÅŸturuluyor...\")\n",
        "merged_dir = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
        "model.save_pretrained_merged(merged_dir, tokenizer, save_method=\"merged_16bit\")\n",
        "print(f\"Merged model (16bit): {merged_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctob6o39M2CS",
        "outputId": "b6a65f98-80b6-42a3-e016-87d0ed99c44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HÄ±zlÄ± inference testi yapÄ±lÄ±yor...\n",
            "\n",
            "TEST Ã‡IKTISI:\n",
            "system\n",
            "You are an expert Python programmer. Please read the problem carefully before writing any Python code.\n",
            "user\n",
            "Write a Python function that takes a list of integers and returns the sum of all even numbers.\n",
            "assistant\n",
            "<think>\n",
            "This is a straightforward problem where I need to find all even numbers in a given list and compute their sum. The key insight is that for each number, I can check if it's divisible by 2 (even) - this is a direct mathematical property. \n",
            "\n",
            "I should consider edge cases first: what if the list is empty? Then the sum would be 0 since there are no numbers to add. What about negative even numbers? The problem doesn't specify whether we include negative numbers or not, but typically when dealing with sums of numbers, we usually assume positive values unless explicitly stated otherwise.\n",
            "\n",
            "Let me test this approach with concrete examples. For [1, 2, 3, 4], the even numbers are 2 and 4, so the sum would be 6. For [-2, 2, -4, 4], both -2 and 2 are even, making the sum 0. Another example might be [1, 3, 5] which has no even numbers, so return 0.\n",
            "\n",
            "The algorithm becomes simple: iterate through each number, check if it's even using modulo operation, and accumulate the sum if true. This handles all cases efficiently with linear time complexity relative to the input size.\n",
            "\n",
            "This approach is optimal because checking divisibility by 2 is constant time per element, and we must examine every element at least once. No alternative approaches like filtering then summing would be less efficient since they'd require additional memory to store filtered results.\n",
            "</think>\n",
            "```python\n",
            "def main():\n",
            "    import sys\n",
            "    data = sys.stdin.read().split()\n",
            "    n = int(data[0])\n",
            "    nums = list(map(int, data[1:1+n]))\n",
            "    \n",
            "    total = 0\n",
            "    for num in nums:\n",
            "        if num % 2 == 0:\n",
            "            total += num\n",
            "            \n",
            "    print(total)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "```\n",
            "\n",
            "GPU Memory KullanÄ±mÄ±:\n",
            "Allocated: 3.25 GB\n",
            "Reserved: 3.35 GB\n",
            "\n",
            "\n",
            "TÃ¼m iÅŸlemler tamamlandÄ±! BaÅŸarÄ±lar!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nHÄ±zlÄ± inference testi yapÄ±lÄ±yor...\\n\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Inference mode\n",
        "\n",
        "test_prompt = \"\"\"Write a Python function that takes a list of integers and returns the sum of all even numbers.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": test_prompt}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.6,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"TEST Ã‡IKTISI:\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\nGPU Memory KullanÄ±mÄ±:\")\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "print(f\"Reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n\\nTÃ¼m iÅŸlemler tamamlandÄ±! BaÅŸarÄ±lar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dpxQ7xRfGT0",
        "outputId": "584346b8-0ee7-4ed6-dd8a-e0da862df468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Analizi\n",
            "\n",
            "Ã–RNEK 1:\n",
            "SORU:\n",
            "Create a function `longer` that accepts a string and sorts the words in it based on their respective lengths in an ascending order. If there are two words of the same lengths, sort them alphabetically...\n",
            "|nMODEL TAHMÄ°NÄ°:\n",
            "<think>\n",
            "This problem requires sorting words by length first (ascending), then alphabetically if lengths are equal. The key insight is that we need to split the input into individual words, process each word individually, then reconstruct the sorted result.\n",
            "\n",
            "Let me break this down into subgoals:\n",
            "1. Split the input string into individual words\n",
            "2. For each word, determine its length and compare with others\n",
            "3. Sort primarily by length (ascending) but secondarily by alphabetical order\n",
            "4. Reconstruct the final string\n",
            "\n",
            "I should consider different approaches. One straightforward approach would be to iterate through all words once, keeping track of both length and original index. Then I could sort using a custom comparator that compares lengths first, then lexicographically if lengths are equal.\n",
            "\n",
            "Alternatively, I could use Python's built-in sorted() function with a lambda key that returns a tuple containing (length, word). This would automatically handle both requirements - primary by length, secondary by alphabetical order.\n",
            "\n",
            "Testing with the given examples shows these approaches work correctly. For \"Another Green World\", \"Green\" has length 5, \"World\" has length 5, so they're sorted alphabetically since \"Green\" comes before \"World\". Similarly for other cases.\n",
            "\n",
            "The built-in approach seems most efficient and concise. It avoids manual iteration and sorting logic, which can be error-prone. The key challenge is handling case sensitivity properly, though the problem states only alphabets, so uppercase priority doesn't matter for comparison.\n",
            "\n",
            "Edge cases: empty strings, single-word inputs, multiple identical words with different lengths, and all-uppercase/lowercase words. The built-in approach handles all these naturally because it uses natural ordering rules.\n",
            "\n",
            "</think>\n",
            "```python\n",
            "def longer(s):\n",
            "    # Split the string into words\n",
            "    words = s.split()\n",
            "    \n",
            "    # Custom sorting key: sort by length (ascending) then by word (alphabetical)\n",
            "    sorted_words = sorted(words, key=lambda x: (len(x), x))\n",
            "    \n",
            "    # Join the sorted words back into a string\n",
            "    return ' '.join(sorted_words)\n",
            "\n",
            "# Read from stdin and output to stdout\n",
            "if __name__ == \"__main__\":\n",
            "    import sys\n",
            "    input_string = sys.stdin.read().strip()\n",
            "    print(longer(input_string))\n",
            "```\n",
            "GERÃ‡EK CEVAP (Referans):\n",
            "def longer(s):\n",
            "    words = s.split()\n",
            "    sorted_words = sorted(words, key=lambda word: (len(word), word))\n",
            "    return ' '.join(sorted_words)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    import sys\n",
            "    input_line = ...\n",
            "\n",
            "Ã–RNEK 2:\n",
            "SORU:\n",
            "Mirko is a huge fan of chess and programming, but typical chess soon became boring for him, so he started having fun with rook pieces.\n",
            "\n",
            "He found a chessboard with $N$ rows and $N$ columns and placed $...\n",
            "|nMODEL TAHMÄ°NÄ°:\n",
            "<think>\n",
            "This problem involves tracking rook movements and calculating attacked fields after each move. The key insight is that we need to track the XOR values between rooks' positions over time, since a field becomes attacked when the XOR of all rooks' powers on that field is positive.\n",
            "\n",
            "First, I recognize that the XOR operation preserves parity - it's commutative and associative, and XORing zeros gives zero. This means if we have multiple rooks with the same power value, they won't affect the XOR result for any field. However, different powers might create dependencies.\n",
            "\n",
            "Let me consider what happens when rooks move: when one rook moves, the XOR value changes for all fields that contain both rooks. Specifically, if we have rooks at positions (r1,c1) and (r2,c2), then the XOR of their powers on a field depends on whether that field contains either rook. If neither rook occupies the field, the XOR remains unchanged. If one rook occupies the field, the XOR flips. If both occupy, the XOR toggles.\n",
            "\n",
            "This suggests we can model this as a state machine where states represent sets of rook positions. The XOR operation is equivalent to flipping bits in the set representation. Since XOR operations commute and are linear, we can think of each rook position as contributing a bit to the final XOR result.\n",
            "\n",
            "Wait, let me verify with a small example. Suppose we have rooks at positions (1,1)=a, (2,2)=b, (1,2)=c. Initially, no field has more than one rook, so XOR=0. Now if we flip c to d, the XOR changes because we're adding another rook. Similarly, flipping b to e would add another rook, flipping a to f would remove a rook.\n",
            "\n",
            "Actually, let me think about this differently. The XOR of rook powers on a field depends on whether that field contains any rook. If no rook occupies the field, XOR=0. If one rook occupies the field, XOR=that single rook's power. If both rooks occupy the field, XOR=0 again? Let me check: if both rooks are at (x,y), then XOR(a,b)=aâŠ•b. But if both are at (x,y), then XOR(a,b)=0, so XOR(a,b,c)=c? Wait, that doesn't match my expectations.\n",
            "\n",
            "I realize I'm misunderstanding. The problem says \"a rook sees all the fields that are in his row or column except its own field.\" So if two rooks are at the same position, they don't see each other directly. But the XOR operation applies regardless of visibility. Actually, let me reconsider.\n",
            "\n",
            "Looking at the sample input 1:\n",
            "Initial: (1,1)=1, (2,2)=1 â†’ XOR=0\n",
            "Then: (2,2)=1 â†’ XOR changes because we have both rooks at (2,2)\n",
            "(2,2)=1 â†’ XOR changes because we have both rooks at (2,2)\n",
            "\n",
            "So the XOR changes even though the rooks are at the same position! This means the XOR operation is not symmetric around visibility. In fact, the XOR operation is actually symmetric - if two rooks are at the same position, they don't share information about that position, so XOR remains the same.\n",
            "\n",
            "But wait, let me test this with a concrete example. Suppose we have rooks at (1,1)=1 and (1,2)=2. Then XOR=3. If I move the rooks to (2,1)=3 and (2,2)=4, the new XOR would still be 3, not 7. So indeed, the XOR operation is NOT symmetric around visibility.\n",
            "\n",
            "Therefore, the XOR operation is defined as follows: for any two rooks at positions (r1,c1) and (r2,c2):\n",
            "- XOR = r1âŠ•r2 âŠ• c1âŠ•c2 (since XOR is commutative and associative)\n",
            "\n",
            "Now I understand why the XOR operation is not affected by visibility - it's just counting bits in the rook coordinates. When rooks are at the same position, XOR remains unchanged.\n",
            "\n",
            "So the state space consists of all possible subsets S of {1,...,N} representing the set of rook rows, and all possible subsets T of {1,...,N} representing the set of rook columns. There are 2^N Ã— 2^N = 4Ã—10^9 states.\n",
            "\n",
            "However, P â‰¤ 100,000 and N up to 1 billion, so we cannot enumerate all states. We need a better approach.\n",
            "\n",
            "Let me think about the constraints. The XOR operation is essentially computing the XOR of all rook powers modulo some prime. But since the rooks can have large powers up to 1 billion, we need to work modulo something.\n",
            "\n",
            "Actually, let me define M = max\n",
            "GERÃ‡EK CEVAP (Referans):\n",
            "import sys\n",
            "\n",
            "def main():\n",
            "    data = sys.stdin.read().split()\n",
            "    it = iter(data)\n",
            "    N = int(next(it)); K = int(next(it)); P = int(next(it))\n",
            "    \n",
            "    rooks = []\n",
            "    row_xor = {}\n",
            "    col_xor = {}\n",
            "    co...\n",
            "\n",
            "Ã–RNEK 3:\n",
            "SORU:\n",
            "In Ciel's restaurant, a waiter is training.\n",
            "Since the waiter isn't good at arithmetic, sometimes he gives guests wrong change.\n",
            "Ciel gives him a simple problem.\n",
            "What is A-B (A minus B) ?\n",
            "\n",
            "\n",
            "Surprisingly...\n",
            "|nMODEL TAHMÄ°NÄ°:\n",
            "<think>\n",
            "This problem requires finding a single-digit difference between two numbers where all digits except one are correct, with exactly one digit being different.\n",
            "\n",
            "First, I need to understand what makes this challenging. The key insight is that we're looking for a number that differs from the actual result by exactly one digit, and it should have the same length as the actual result.\n",
            "\n",
            "Let me break this down into subgoals:\n",
            "1. Find the correct answer: A - B\n",
            "2. Generate candidate answers by changing exactly one digit of the correct answer\n",
            "3. Ensure each candidate has the same number of digits as the correct answer\n",
            "4. Verify that only one digit differs from the correct answer\n",
            "\n",
            "For example, given 5858 and 1234, the correct answer is 4624. We can generate candidates like:\n",
            "- Change '4' to something else: 5624, 5724, 5834, etc.\n",
            "- Change '6' to something else: 5724, 5824, 5834, etc.\n",
            "- Change '2' to something else: 5624, 5724, 5834, etc.\n",
            "- Change '6' to '2': 5724\n",
            "- Change '6' to '3': 5734\n",
            "\n",
            "We need to find which of these has exactly one digit difference from 4624.\n",
            "\n",
            "I notice that the correct answer always has the same number of digits as A and B. This means we don't need to worry about padding leading zeros.\n",
            "\n",
            "Now let me consider approaches:\n",
            "\n",
            "Approach 1: Brute force - try all possible single-digit changes for each position in the string representation\n",
            "- Convert both numbers to strings\n",
            "- For each position i in the string representation\n",
            "- Try changing the digit at position i to any valid digit (0-9)\n",
            "- Check if the resulting number equals the correct answer after removing the changed digit\n",
            "- If yes, return that number\n",
            "\n",
            "Wait, this seems complex. Let me think differently.\n",
            "\n",
            "Actually, since the numbers are small (â‰¤10000), we can use a simpler approach: we know the correct answer has the same number of digits as A and B. So we can iterate through all positions where the digits differ, and try changing each digit to create potential solutions.\n",
            "\n",
            "But we need to ensure the solution has exactly one digit difference. Since we're dealing with strings, we can compare the strings directly.\n",
            "\n",
            "Let me test with a concrete example: A=5858, B=1234\n",
            "Correct answer = \"4624\"\n",
            "Possible differences could include:\n",
            "- Changing the last digit of 4624 to 5? But then we'd get 5624 which would be too large (since A=5858)\n",
            "- Or maybe changing the middle digit? Let's check: \"462\" vs \"562\"? That doesn't work because we'd get 562 which is smaller than 4624.\n",
            "\n",
            "Actually, I recall that when we're trying to find a number with exactly one digit different, we typically look for the number formed by replacing the differing digit with another digit.\n",
            "\n",
            "So my approach: convert both numbers to strings, find the first position where they differ, then try changing that digit to every other possible digit (except the original digit itself).\n",
            "\n",
            "Let me verify with a simple case: A=100, B=1\n",
            "Correct: 99\n",
            "Differences: \n",
            "- Position 0: '1'â†’'9'? No, 99â‰ 100\n",
            "- Position 1: '0'â†’'9'? No, 99â‰ 100  \n",
            "- Position 2: '0'â†’'1'? Yes! Then we get 110 which is incorrect.\n",
            "\n",
            "Wait, that's not right either. Let me reconsider.\n",
            "\n",
            "Actually, I think I misunderstood. The problem says \"exactly one wrong digit\" but the sample output shows 1624 for 5858-1234. Let me check again.\n",
            "\n",
            "The sample says: \"the correct answer is 4624. So, for instance, 2624, 4324, 4623, 4604 and 4629 will be accepted\"\n",
            "\n",
            "This suggests that 4624 might be the correct answer, but the sample shows 1624 instead. Maybe I'm reading too quickly or misunderstanding.\n",
            "\n",
            "Let me recalculate 5858-1234:\n",
            "5858 - 1234 = 4624 âœ“\n",
            "\n",
            "Oh no, I made a mistake. Let me recompute properly:\n",
            "5858 - 1234 = 4624 âœ“\n",
            "\n",
            "I see now. The sample shows\n",
            "GERÃ‡EK CEVAP (Referans):\n",
            "def main():\n",
            "    A, B = map(int, input().split())\n",
            "    correct = A - B\n",
            "    correct_str = str(correct)\n",
            "    \n",
            "    # Try changing each digit position\n",
            "    for i in range(len(correct_str)):\n",
            "        # For each...\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "num_samples = 3\n",
        "test_indices = random.sample(range(len(dataset[\"test\"])), num_samples)\n",
        "test_samples = dataset[\"test\"].select(test_indices)\n",
        "\n",
        "print(\"Test Analizi\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "  input_text = sample[\"input\"]\n",
        "  ground_truth = sample[\"solution\"]\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "      {\"role\": \"user\", \"content\": input_text}\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "  )\n",
        "\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  print(f\"\\nÃ–RNEK {i+1}:\")\n",
        "  print(f\"SORU:\\n{input_text[:200]}...\")\n",
        "  print(f\"|nMODEL TAHMÄ°NÄ°:\\n{prediction.split('assistant')[-1].strip()}\")\n",
        "  print(f\"GERÃ‡EK CEVAP (Referans):\\n{ground_truth[:200]}...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}