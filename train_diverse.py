# -*- coding: utf-8 -*-
"""lora_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/102Zb350qoV1b2d21tdSczyvi2ZVt0go7
"""

!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" -q
!pip install --no-deps trl peft accelerate bitsandbytes -q
!pip install --upgrade --force-reinstall pyarrow
!pip install datasets==4.3.0

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/datasets

!git clone https://huggingface.co/datasets/Naholav/CodeGen-Deep-5K

!git clone https://huggingface.co/datasets/Naholav/CodeGen-Diverse-5K

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/model

!git clone https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/NLP-LLM-FINETUNING/LORA

import os
import json
import torch
from datetime import datetime
from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments, EarlyStoppingCallback
from transformers.trainer_callback import TrainerCallback

TRAINING_MODE = "diverse"

USE_REASONING = False

DEEP_DATASET = "Naholav/CodeGen-Deep-5K"
DIVERSE_DATASET = "Naholav/CodeGen-Diverse-5K"

if USE_REASONING:
  SYSTEM_PROMPT = "You are an expert Python programmer. Please read the problem carefully before writing any Python code."
else:
  SYSTEM_PROMPT = "You are an expert programmer. Use <think> tags for reasoning before writing code."

LORA_R = 32
LORA_ALPHA = 64  # r * 2
LORA_DROPOUT = 0.1

LEARNING_RATE = 3e-4
EPOCHS = 3
BATCH_SIZE = 16
GRADIENT_ACCUMULATION = 1
MAX_LENGTH = 1024 if not USE_REASONING else 8192

WEIGHT_DECAY = 0.05
WARMUP_RATIO = 0.03
SEED = 42

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
DIRECTORY_PATH = "/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA"
LOG_DIR = f"{DIRECTORY_PATH}/logs/{TRAINING_MODE}/{timestamp}"
OUTPUT_DIR = f"{DIRECTORY_PATH}/training_models/{TRAINING_MODE}/{timestamp}"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

class LogCallback(TrainerCallback):
    """Write logs to both screen and file"""
    def __init__(self, log_file):
        self.log_file = log_file

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            with open(self.log_file, 'a') as f:
                log_entry = {
                    "step": state.global_step,
                    "epoch": round(state.epoch, 2) if state.epoch else 0,
                    **logs
                }
                f.write(json.dumps(log_entry) + "\n")

            if "loss" in logs:
                print(f"ðŸ“Š [Step {state.global_step}] Train Loss: {logs['loss']:.4f}")
            if "eval_loss" in logs:
                print(f"ðŸ“ˆ [Step {state.global_step}] Eval Loss: {logs['eval_loss']:.4f}")

print("Model loading...")

max_seq_length = MAX_LENGTH
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-Coder-1.5B-Instruct",
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=False,
)

print("Model loaded!")
print(f"Max sequence length: {max_seq_length}")
print(f"Device: {model.device}")

FastLanguageModel.for_inference(model)
if not USE_REASONING:
  messages = [
      {"role": "system", "content": "You are an expert Python programmer. Please read the problem carefully before writing any Python code."},
      {"role": "user", "content": "Write a Python function to check if a number is prime."}
  ]
else:
  messages = [
      {"role": "system", "content": "You are an expert programmer. Use <think> tags for reasoning before writing code."},
      {"role": "user", "content": "Write a Python function to check if a number is prime."}
  ]

inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
outputs = model.generate(input_ids=inputs, max_new_tokens=1024)
print("Base Model Answer: ")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

print("\nLoRA applying...")
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=SEED,
)

print("LoRA applied!")

print("Dataset loading...")
dataset_name = DEEP_DATASET if TRAINING_MODE == "deep" else DIVERSE_DATASET
print(f"Dataset: {dataset_name}")

dataset = load_dataset(dataset_name)

dataset = dataset["train"].train_test_split(test_size=0.1, seed=42)
print(f"Train: {len(dataset['train'])} samples")
print(f"Test: {len(dataset['test'])} samples")

def formatting_func(examples):
    inputs = examples["input"]
    outputs = examples["output"] if USE_REASONING else examples["solution"]

    texts = []

    for input_text, output_text in zip(inputs, outputs):
        text = f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n"
        text += f"<|im_start|>user\n{input_text}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{output_text}<|im_end|>"
        texts.append(text)

    return texts

print("Training ayarlarÄ± hazÄ±rlanÄ±yor...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,

    # Optimizer & Scheduler
    learning_rate=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY,
    warmup_ratio=WARMUP_RATIO,
    optim="adamw_8bit",
    lr_scheduler_type="cosine",

    # Logging & Saving
    logging_steps=20,
    eval_steps=100,
    save_steps=100,
    save_total_limit=3,

    # Evaluation & Model Selection
    eval_strategy="steps",
    save_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    # Optimization
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    max_grad_norm=1.0,

    # Misc
    report_to="tensorboard",
    seed=SEED,
    dataloader_num_workers=2,
)

print(f"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"Precision: {'BF16' if is_bfloat16_supported() else 'FP16'}")

print("\nTrainer preparing...")
log_file = os.path.join(LOG_DIR, "training_log.jsonl")
log_callback = LogCallback(log_file)
early_stopping = EarlyStoppingCallback(early_stopping_patience=2)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    max_seq_length=max_seq_length,
    args=training_args,
    callbacks=[log_callback, early_stopping],
    packing=False,
    formatting_func=formatting_func
)

config = {
    "training_mode": TRAINING_MODE,
    "dataset": dataset_name,
    "use_reasoning": USE_REASONING,
    "base_model": "Qwen2.5-Coder-1.5B-Instruct",
    "framework": "unsloth",

    "lora_config": {
        "r": LORA_R,
        "alpha": LORA_ALPHA,
        "dropout": LORA_DROPOUT,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"]
    },

    "training_config": {
        "learning_rate": LEARNING_RATE,
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "gradient_accumulation": GRADIENT_ACCUMULATION,
        "effective_batch_size": BATCH_SIZE * GRADIENT_ACCUMULATION,
        "max_length": MAX_LENGTH,
        "weight_decay": WEIGHT_DECAY,
        "warmup_ratio": WARMUP_RATIO,
        "optimizer": "adamw_8bit",
        "scheduler": "cosine",
        "early_stopping_patience": 2,
        "seed": SEED
    }
}

with open(os.path.join(OUTPUT_DIR, "config.json"), 'w') as f:
    json.dump(config, f, indent=2)

print("Trainer is ready!")

print("Training is starting!")
gpu_stats = torch.cuda.get_device_properties(0)
print(f"GPU: {gpu_stats.name}")
print(f"Total Memory: {round(gpu_stats.total_memory / 1024 / 1024 / 1024, 2)} GB")
print(f"Compute Capability: {gpu_stats.major}.{gpu_stats.minor}\n")

trainer_stats = trainer.train()

print("Training completed!")

print("\nModel saving...")
lora_dir = os.path.join(DIRECTORY_PATH, "lora_adapter")
model.save_pretrained(lora_dir)
tokenizer.save_pretrained(lora_dir)
print(f"LoRA adapter: {lora_dir}")

print("\nMerged model creating...")
merged_dir = os.path.join(OUTPUT_DIR, "merged_model")
model.save_pretrained_merged(merged_dir, tokenizer, save_method="merged_16bit")
print(f"Merged model (16bit): {merged_dir}")

print("\nFast inference test doing...\n")

FastLanguageModel.for_inference(model)  # Inference mode

test_prompt = """Write a Python function that takes a list of integers and returns the sum of all even numbers."""

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": test_prompt}
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
).to("cuda")

outputs = model.generate(
    input_ids=inputs,
    max_new_tokens=1024,
    temperature=0.6,
    do_sample=True,
    top_p=0.9,
    pad_token_id=tokenizer.pad_token_id
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("TEST Ã‡IKTISI:")
print(response)

print("\nGPU Memory KullanÄ±mÄ±:")
print(f"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB")
print(f"Reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB")

print("\n\nAll processes completed! Congratulations")

import random

num_samples = 3
test_indices = random.sample(range(len(dataset["test"])), num_samples)
test_samples = dataset["test"].select(test_indices)

print("Test Analysis")

FastLanguageModel.for_inference(model)

for i, sample in enumerate(test_samples):
  input_text = sample["input"]
  ground_truth = sample["solution"]

  messages = [
      {"role": "system", "content": SYSTEM_PROMPT},
      {"role": "user", "content": input_text}
  ]

  inputs = tokenizer.apply_chat_template(
      messages,
      tokenize=True,
      add_generation_prompt=True,
      return_tensors="pt"
  ).to("cuda")

  outputs = model.generate(
        input_ids=inputs,
        max_new_tokens=1024,
        temperature=0.6,
        top_p=0.9,
        do_sample=True
  )

  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)

  print(f"\nSAMPLE {i+1}:")
  print(f"QUESTION:\n{input_text[:200]}...")
  print(f"|nMODEL PREDICTION:\n{prediction.split('assistant')[-1].strip()}")
  print(f"REAL ANSWER (Referans):\n{ground_truth[:200]}...")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/NLP-LLM-FINETUNING/LORA/logs"

